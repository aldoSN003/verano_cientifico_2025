# -*- coding: utf-8 -*-
"""augmented-vit-lemon-classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1utmLZc0Uzxaklueg4HEIqDR9MTMMJNqJ
"""

import os
import numpy as np
import torch
from torch.utils.data import DataLoader
from transformers import (
    AutoImageProcessor,
    AutoModelForImageClassification,
    TrainingArguments,
    Trainer,
    pipeline
)
from datasets import load_dataset
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from huggingface_hub import login
import json

import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")

!pip install -U datasets

from huggingface_hub import notebook_login
notebook_login()

# ============================================================================
# CLASIFICACIÓN DE ENFERMEDADES DE LIMÓN USANDO VISION TRANSFORMER (ViT)
# CON AUMENTO DE DATOS
# ============================================================================

# ## 1. IMPORTAR LIBRERÍAS REQUERIDAS
import torch
import torch.nn as nn
import torchvision.transforms as T
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import json
import pandas as pd
from PIL import Image
from datasets import load_dataset
from transformers import (
    AutoImageProcessor,
    AutoModelForImageClassification,
    Trainer,
    TrainingArguments,
    pipeline
)
from sklearn.metrics import (
    accuracy_score,
    precision_recall_fscore_support,
    confusion_matrix
)
from huggingface_hub import login

# Configurar estilo para mejores gráficos
plt.style.use('default')
sns.set_palette("husl")

# ## 2. DEFINICIÓN DE LA CLASE CLASIFICADOR DE ENFERMEDADES DE LIMÓN
class LemonDiseaseClassifier:
    def __init__(self, dataset_name="AldoSN/lemon-leaf-disease-dataset", model_name="google/vit-base-patch16-224"):
        self.dataset_name = dataset_name
        self.model_name = model_name
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Etiquetas de clase para tu dataset
        self.class_labels = [
            "Anthracnose", "Bacterial Blight", "Citrus Canker", "Curl Virus",
            "Deficiency Leaf", "Dry Leaf", "Healthy Leaf", "Sooty Mould", "Spider Mites"
        ]

        # Inicializar componentes
        self.processor = None
        self.model = None
        self.dataset = None
        self.trainer = None

        # Inicializar transformaciones de aumento
        self.augmentation_transform = self._create_augmentation_pipeline()

        print(f"Inicializando Clasificador de Enfermedades de Limón")
        print(f"Dataset: {self.dataset_name}")
        print(f"Modelo: {self.model_name}")
        print(f"Dispositivo: {self.device}")
        print(f"Aumento de Datos: Habilitado")

    def _create_augmentation_pipeline(self):
        """Crear pipeline de aumento con parámetros especificados"""
        return T.Compose([
            # Volteo horizontal
            T.RandomHorizontalFlip(p=0.5),

            # Rotación - 90° en sentido horario (usando negativo para horario)
            T.RandomApply([T.RandomRotation(degrees=(-90, -90))], p=0.5),

            # Zoom (Random Resized Crop simula zoom)
            T.RandomResizedCrop(
                size=(224, 224),
                scale=(0.64, 1.44),  # 0.8² a 1.2² para escalado de área
                ratio=(0.95, 1.05),  # Mantener relación de aspecto
            ),

            # Desplazamiento de altura - 10% de la altura de imagen
            T.RandomApply([
                T.RandomAffine(
                    degrees=0,
                    translate=(0, 0.1),  # (horizontal, vertical) traslación
                    scale=None,
                    shear=None
                )
            ], p=0.5),

            # Corte - factor de corte de 0.1 a 0.5
            T.RandomApply([
                T.RandomAffine(
                    degrees=0,
                    translate=None,
                    scale=None,
                    shear=(-28.6, 28.6)  # Convertir factor a grados (0.5 rad ≈ 28.6°)
                )
            ], p=0.5),

            # Ajuste de brillo - 0.5 a 1.5
            T.ColorJitter(brightness=(0.5, 1.5), contrast=0, saturation=0, hue=0),

            # Convertir a tensor para adición de ruido
            T.ToTensor(),

            # Agregar ruido gaussiano (media=0, std=25/255)
            T.Lambda(lambda x: x + torch.randn_like(x) * (25/255)),

            # Fijar valores al rango válido
            T.Lambda(lambda x: torch.clamp(x, 0, 1)),

            # Convertir de vuelta a PIL para el procesador
            T.ToPILImage()
        ])

    def load_dataset_and_processor(self):
        """Cargar dataset e inicializar procesador"""
        print("Cargando dataset...")
        self.dataset = load_dataset(self.dataset_name)

        print("Cargando procesador...")
        self.processor = AutoImageProcessor.from_pretrained(self.model_name)

        print(f"Dataset cargado - Entrenamiento: {len(self.dataset['train'])}, Prueba: {len(self.dataset['test'])}")

        return self.dataset, self.processor

    def preprocess_data(self):
        """Preprocesar imágenes para entrenamiento con aumento"""
        print("Preprocesando datos con aumento...")

        def preprocess_function(examples):
            # Procesar imágenes con aumento para conjunto de entrenamiento
            images = []
            for image in examples["image"]:
                # Convertir a RGB si es necesario
                image = image.convert("RGB")

                # Aplicar aumento solo a datos de entrenamiento
                if hasattr(examples, '_split') and examples._split == 'train':
                    # Aplicar aumento
                    image = self.augmentation_transform(image)

                images.append(image)

            inputs = self.processor(images, return_tensors="pt")
            inputs["labels"] = examples["label"]
            return inputs

        # Aplicar preprocesamiento
        self.dataset = self.dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=["image"]
        )

        # Establecer formato para PyTorch
        self.dataset.set_format("torch")

        print("Preprocesamiento de datos completo")
        return self.dataset

    def initialize_model(self):
        """Inicializar el modelo para ajuste fino"""
        print("Inicializando modelo...")

        # Cargar modelo con número correcto de etiquetas
        self.model = AutoModelForImageClassification.from_pretrained(
            self.model_name,
            num_labels=len(self.class_labels),
            id2label={i: label for i, label in enumerate(self.class_labels)},
            label2id={label: i for i, label in enumerate(self.class_labels)},
            ignore_mismatched_sizes=True
        )
        self.model = self.model.to(self.device)

        print(f"Modelo inicializado con {len(self.class_labels)} clases")
        return self.model

    def compute_metrics(self, eval_pred):
        """Calcular métricas de evaluación"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)

        # Calcular métricas
        accuracy = accuracy_score(labels, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')

        return {
            'accuracy': accuracy,
            'f1': f1,
            'precision': precision,
            'recall': recall
        }

    def setup_training_args(self, output_dir="./augmented-vit-lemon-classifier", num_epochs=10, batch_size=64):
        """Configurar argumentos de entrenamiento"""
        return TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            warmup_steps=500,
            weight_decay=0.01,
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            eval_strategy="epoch",
            save_strategy="epoch",
            load_best_model_at_end=True,
            metric_for_best_model="f1",
            greater_is_better=True,
            remove_unused_columns=False,
            push_to_hub=False,
            dataloader_pin_memory=False,

            run_name="augmented-vit-lemon-classifier"
        )

    def train_model(self, training_args):
        """Entrenar el modelo"""
        print("Iniciando entrenamiento...")

        # Inicializar entrenador
        self.trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=self.dataset["train"],
            eval_dataset=self.dataset["test"],
            compute_metrics=self.compute_metrics,
            tokenizer=self.processor,  # Para guardar preprocesamiento
        )

        # Entrenar el modelo
        train_result = self.trainer.train()

        # Guardar el modelo
        self.trainer.save_model()

        print("Entrenamiento completado")

        # Guardar historial de entrenamiento para graficar después
        self.save_training_history(train_result, training_args.output_dir)

        return train_result

    def evaluate_model(self, output_dir="./vit-lemon-disease"):
        """Evaluación completa del modelo con métricas detalladas"""
        print("Evaluando modelo...")

        # Obtener predicciones
        predictions = self.trainer.predict(self.dataset["test"])
        y_pred = np.argmax(predictions.predictions, axis=1)
        y_true = predictions.label_ids

        # Calcular métricas detalladas
        accuracy = accuracy_score(y_true, y_pred)
        precision, recall, f1, support = precision_recall_fscore_support(
            y_true, y_pred, average=None, labels=range(len(self.class_labels))
        )

        # Métricas generales
        precision_weighted = precision_recall_fscore_support(y_true, y_pred, average='weighted')[0]
        recall_weighted = precision_recall_fscore_support(y_true, y_pred, average='weighted')[1]
        f1_weighted = precision_recall_fscore_support(y_true, y_pred, average='weighted')[2]

        # Imprimir resultados
        print(f"\nRESULTADOS DE EVALUACIÓN:")
        print(f"=" * 50)
        print(f"Precisión General: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"Precisión Ponderada: {precision_weighted:.4f}")
        print(f"Recall Ponderado: {recall_weighted:.4f}")
        print(f"F1-Score Ponderado: {f1_weighted:.4f}")
        print(f"=" * 50)

        # Métricas por clase
        print(f"\nMÉTRICAS POR CLASE:")
        print(f"{'Clase':<20} {'Precisión':<10} {'Recall':<10} {'F1-Score':<10} {'Soporte':<10}")
        print("-" * 70)
        for i, (class_name, prec, rec, f1_score, supp) in enumerate(zip(self.class_labels, precision, recall, f1, support)):
            print(f"{class_name:<20} {prec:<10.4f} {rec:<10.4f} {f1_score:<10.4f} {supp:<10}")

        # Matriz de Confusión
        cm = confusion_matrix(y_true, y_pred)
        self.plot_confusion_matrix(cm, output_dir)

        # Guardar métricas en archivo
        metrics_dict = {
            "overall_metrics": {
                "accuracy": float(accuracy),
                "precision": float(precision_weighted),
                "recall": float(recall_weighted),
                "f1_score": float(f1_weighted)
            },
            "per_class_metrics": {
                self.class_labels[i]: {
                    "precision": float(precision[i]),
                    "recall": float(recall[i]),
                    "f1_score": float(f1[i]),
                    "support": int(support[i])
                }
                for i in range(len(self.class_labels))
            }
        }

        with open(f"{output_dir}/evaluation_metrics.json", "w") as f:
            json.dump(metrics_dict, f, indent=2)

        print(f"\nMétricas guardadas en {output_dir}/evaluation_metrics.json")
        print(f"Matriz de confusión guardada en {output_dir}/confusion_matrix.png")

        return metrics_dict

    def plot_confusion_matrix(self, cm, output_dir):
        """Graficar y guardar matriz de confusión"""
        plt.figure(figsize=(12, 10))
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=self.class_labels,
            yticklabels=self.class_labels,
            cbar_kws={'label': 'Cantidad'}
        )
        plt.title('Matriz de Confusión - Clasificación de Enfermedades de Limón', fontsize=16, fontweight='bold')
        plt.xlabel('Etiqueta Predicha', fontsize=12)
        plt.ylabel('Etiqueta Verdadera', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.savefig(f"{output_dir}/confusion_matrix.png", dpi=300, bbox_inches='tight')
        plt.show()


    def save_training_history(self, train_result, output_dir):
        """Guardar historial de entrenamiento para graficar después"""
        import pandas as pd

        # Extraer logs de entrenamiento
        log_history = self.trainer.state.log_history

        # Separar logs de entrenamiento y evaluación
        train_logs = []
        eval_logs = []

        for log in log_history:
            if 'loss' in log and 'eval_loss' not in log:
                train_logs.append(log)
            elif 'eval_loss' in log:
                eval_logs.append(log)

        # Guardar en JSON para uso posterior
        training_history = {
            'train_logs': train_logs,
            'eval_logs': eval_logs
        }

        with open(f"{output_dir}/training_history.json", "w") as f:
            json.dump(training_history, f, indent=2)

        print(f"Historial de entrenamiento guardado en {output_dir}/training_history.json")

        return training_history

    def plot_training_curves(self, output_dir="./vit-lemon-disease"):
        """Graficar curvas de entrenamiento y pérdida/precisión de validación"""
        import json
        import pandas as pd

        # Cargar historial de entrenamiento
        try:
            with open(f"{output_dir}/training_history.json", "r") as f:
                history = json.load(f)
        except FileNotFoundError:
            print("Historial de entrenamiento no encontrado. Asegúrate de entrenar el modelo primero.")
            return

        # Extraer datos
        train_logs = history['train_logs']
        eval_logs = history['eval_logs']

        # Crear DataFrames
        train_df = pd.DataFrame(train_logs)
        eval_df = pd.DataFrame(eval_logs)

        # Calcular épocas para logs de entrenamiento (convertir pasos a épocas)
        if not train_df.empty and 'step' in train_df.columns:
            # Estimar pasos por época de logs de evaluación
            if not eval_df.empty and len(eval_df) > 1:
                total_steps = train_df['step'].max()
                total_epochs = eval_df['epoch'].max()
                steps_per_epoch = total_steps / total_epochs
                train_df['epoch'] = train_df['step'] / steps_per_epoch
            else:
                # Respaldo: asumir que entrenamiento se registró cada 10 pasos y estimar
                train_df['epoch'] = train_df['step'] / (len(train_df) / 10)

        # Crear subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # Graficar Pérdida de Entrenamiento (ahora en épocas)
        if 'loss' in train_df.columns and 'epoch' in train_df.columns:
            ax1.plot(train_df['epoch'], train_df['loss'], 'b-', label='Training Loss', linewidth=2, alpha=0.7)
            ax1.set_title('Training Loss', fontsize=14, fontweight='bold')
            ax1.set_xlabel('Epoch')
            ax1.set_ylabel('Loss')
            ax1.grid(True, alpha=0.3)
            ax1.legend()

        # Graficar Pérdida de Validación
        if 'eval_loss' in eval_df.columns:
            ax2.plot(eval_df['epoch'], eval_df['eval_loss'], 'r-', label='Validation Loss', linewidth=2, marker='o')
            ax2.set_title('Validation Loss', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Epoch')
            ax2.set_ylabel('Loss')
            ax2.grid(True, alpha=0.3)
            ax2.legend()

        # Graficar Pérdida de Entrenamiento y Validación Juntas
        if 'loss' in train_df.columns and 'eval_loss' in eval_df.columns:
            ax3.plot(train_df['epoch'], train_df['loss'], 'b-', label='Training Loss', linewidth=2, alpha=0.7)
            ax3.plot(eval_df['epoch'], eval_df['eval_loss'], 'r-', label='Validation Loss', linewidth=2, marker='o')
            ax3.set_title('Loss: Training vs Validation', fontsize=14, fontweight='bold')
            ax3.set_xlabel('Epoch')
            ax3.set_ylabel('Loss')
            ax3.grid(True, alpha=0.3)
            ax3.legend()

        # Graficar Métricas de Validación (Precisión y F1)
        if 'eval_accuracy' in eval_df.columns and 'eval_f1' in eval_df.columns and 'epoch' in eval_df.columns:
            ax4.plot(eval_df['epoch'], eval_df['eval_accuracy'], 'g-', label='Validation Accuracy', linewidth=2, marker='s')
            ax4.plot(eval_df['epoch'], eval_df['eval_f1'], 'purple', label='Validation F1-Score', linewidth=2, marker='^')
            ax4.set_title('Validation Metrics', fontsize=14, fontweight='bold')
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Score')
            ax4.grid(True, alpha=0.3)
            ax4.legend()
        elif 'eval_accuracy' in eval_df.columns and 'epoch' in eval_df.columns:
            ax4.plot(eval_df['epoch'], eval_df['eval_accuracy'], 'g-', label='Validation Accuracy', linewidth=2, marker='s')
            ax4.set_title('Validation Accuracy', fontsize=14, fontweight='bold')
            ax4.set_xlabel('Epoch')
            ax4.set_ylabel('Accuracy')
            ax4.grid(True, alpha=0.3)
            ax4.legend()

        plt.tight_layout()
        plt.savefig(f"{output_dir}/training_curves.png", dpi=300, bbox_inches='tight')
        plt.show()

        print(f"Curvas de entrenamiento guardadas en {output_dir}/training_curves.png")

    def plot_combined_metrics(self, output_dir="./vit-lemon-disease"):
        """Graficar métricas combinadas de entrenamiento y validación"""
        import json
        import pandas as pd

        # Cargar historial de entrenamiento
        try:
            with open(f"{output_dir}/training_history.json", "r") as f:
                history = json.load(f)
        except FileNotFoundError:
            print("Historial de entrenamiento no encontrado. Asegúrate de entrenar el modelo primero.")
            return

        eval_logs = history['eval_logs']
        eval_df = pd.DataFrame(eval_logs)

        # Crear subplot para métricas combinadas
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # Gráfico de Pérdida Combinada
        if 'eval_loss' in eval_df.columns:
            ax1.plot(eval_df['epoch'], eval_df['eval_loss'], 'r-', label='Pérdida de Validación', linewidth=2, marker='o')
            ax1.set_title('Progreso de Entrenamiento - Pérdida', fontsize=14, fontweight='bold')
            ax1.set_xlabel('Época')
            ax1.set_ylabel('Pérdida')
            ax1.grid(True, alpha=0.3)
            ax1.legend()

        # Gráfico de Precisión/F1 Combinado
        if 'eval_accuracy' in eval_df.columns and 'eval_f1' in eval_df.columns:
            ax2.plot(eval_df['epoch'], eval_df['eval_accuracy'], 'g-', label='Precisión de Validación', linewidth=2, marker='s')
            ax2.plot(eval_df['epoch'], eval_df['eval_f1'], 'purple', label='F1-Score de Validación', linewidth=2, marker='^')
            ax2.set_title('Progreso de Entrenamiento - Métricas', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Época')
            ax2.set_ylabel('Puntuación')
            ax2.grid(True, alpha=0.3)
            ax2.legend()

        plt.tight_layout()
        plt.savefig(f"{output_dir}/combined_metrics.png", dpi=300, bbox_inches='tight')
        plt.show()

        print(f"Gráfico de métricas combinadas guardado en {output_dir}/combined_metrics.png")

# ## 3. PIPELINE PRINCIPAL DE ENTRENAMIENTO 

print("Clasificación de Enfermedades de Limón - Pipeline de Ajuste Fino ViT")
print("=" * 60)

# Inicializar clasificador
classifier = LemonDiseaseClassifier()

# Cargar y preprocesar datos
classifier.load_dataset_and_processor()
classifier.preprocess_data()

# Inicializar modelo
classifier.initialize_model()

# Configurar argumentos de entrenamiento
training_args = classifier.setup_training_args(
    output_dir="./vit-lemon-disease-final",
    num_epochs=10,  # Ajustar según sea necesario
    batch_size=64   # Ajustar basado en memoria de GPU
)

# Entrenar el modelo
classifier.train_model(training_args)

# Evaluar el modelo
metrics = classifier.evaluate_model("./vit-lemon-disease-final")

# Graficar curvas de entrenamiento
print("Generando curvas de entrenamiento...")
classifier.plot_training_curves("./vit-lemon-disease-final")
classifier.plot_combined_metrics("./vit-lemon-disease-final")
